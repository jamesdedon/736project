{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.80.102.113:4040\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1543851031421)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4f963097\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 2.3.1\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@2c0b8ca8\n",
       "import sqlContext.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DF that references the Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pqSC: org.apache.spark.sql.DataFrame = [value: array<string>]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pqSC = sqlContext.read.parquet(\"data.parquet\")\n",
    "pqScoreCard10Year.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[UNITID, OPEID, O...|\n",
      "|[100654, 00100200...|\n",
      "|[100663, 00105200...|\n",
      "|[100690, 02503400...|\n",
      "|[100706, 00105500...|\n",
      "|[100724, 00100500...|\n",
      "|[100751, 00105100...|\n",
      "|[100760, 00100700...|\n",
      "|[100812, 00100800...|\n",
      "|[100830, 00831000...|\n",
      "|[100858, 00100900...|\n",
      "|[100937, 00101200...|\n",
      "|[101028, 01218200...|\n",
      "|[101073, 01055400...|\n",
      "|[101116, 01303906...|\n",
      "|[101143, 00101500...|\n",
      "|[101161, 00106000...|\n",
      "|[101189, 00100300...|\n",
      "|[101240, 00101700...|\n",
      "|[101277, 04187200...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pqSC.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# St Thomas RDD  \n",
    "This RDD contains all of the St. Thomas results over the years in RDD and dataframe format.\n",
    "\n",
    "Bummed I have to switch back to SQL for this stuff...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "46: error: value update is not a member of org.apache.spark.sql.DataFrame",
     "output_type": "error",
     "traceback": [
      "<console>:46: error: value update is not a member of org.apache.spark.sql.DataFrame",
      "       val pqUst = pqScoreCard10Year.filter(pqScoreCard10Year(\"UNITID\") = \"174914\")",
      "                                            ^",
      ""
     ]
    }
   ],
   "source": [
    "val ust = scoreCard10Year.filter(x => x(0) contains \"174914\")\n",
    "// df.filter(df(\"age\") > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ust.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ust.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ustHeader = header2016.union(ust)\n",
    "// ustHeader.map(_.mkString(\",\")).coalesce(1).saveAsTextFile(\"ustHeader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# St Thomas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Cannot resolve column name \"UNITID\" among (value);",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Cannot resolve column name \"UNITID\" among (value);",
      "  at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:222)",
      "  at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:222)",
      "  at scala.Option.getOrElse(Option.scala:121)",
      "  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)",
      "  at org.apache.spark.sql.Dataset.col(Dataset.scala:1241)",
      "  at org.apache.spark.sql.Dataset.apply(Dataset.scala:1208)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "val ustDF = pqSC.filter(pqSC(\"UNITID\") === \"174914\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Long = 0\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ustDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Peers RDD\n",
    "\n",
    "This RDD filters region 4 (upper midwest/plains), Schools that offer graduate degrees as the highest degree, and a carnegie classification of 4 year, medium size, highly residential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val regionalPeers = scoreCard10Year.filter(x => x(18) == \"4\").filter(x => x(15) == \"4\").filter(x => x(25) == \"13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalPeers.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average SAT score for peers in the region. Ours is 1204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreCard10Year.map(x => x(1899)).map(_.mkString(\"\")).take(1) // I verified index 1899 in all of my load RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalPeers.filter(x => x(1899) == \"2016\").map(x => x(59)).filter(x => x != \"NULL\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalPeers.map(x => x(59)).filter(x => x != \"NULL\").map(_.toInt).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an index of the header "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really had a tough time figuring out how to get the header loaded as just a string. Loading it from the original source results in type RDD[Array[String]], so I ended up using a CSV file that I outputed from the original input. Ended up just needing a **flatMap** to keep it in the correct format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minnesota Schools  \n",
    "This rdd contains all of the Minnesota universities for a statewide comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mnColleges = scoreCard10Year.filter( x => x(5) contains \"MN\").filter( x => x(1899) contains \"2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mnColLoc = mnColleges.filter(x => x(1899) contains \"2016\").filter(x => x(36) != \"NULL\").map(x => x(3) + \",\" + x(21) + \",\" + x(22) + \",\" + x(36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// mnColLoc.take(5).foreach(println) // Only needs to run once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and export RDD to CSV for mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mnColLocHead = Array(\"Name,Lat,Long,AdmRate\")\n",
    "val mnLocRDD = sc.parallelize(mnColLocHead)\n",
    "val mnColLocFin = mnLocRDD.union(mnColLoc)\n",
    "//mnColLocFin.coalesce(1).saveAsTextFile(\"MnSchools\") // only run once"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%python\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "//mnAdmDf = pd.read_csv(\"mnColLocAdm.csv\")\n",
    "\n",
    "fig = pt.figure(figsize=(8, 8))\n",
    "m = Basemap(projection= 'lcc', resolution=None,\n",
    "           width=8E6, height=8E6,\n",
    "           lat_0=44, lon_0=-93,)\n",
    "m.etopo(scale=0.5, alpha=0.5)\n",
    "\n",
    "# Map (long, lat) to (x, y) for plotting\n",
    "x, y = m(-93.188986, 44.943439)\n",
    "plt.plot(x, y, 'ok', markersize=5)\n",
    "plt.text(x, y, 'UniversityofStThomas', fontsize=12);\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
